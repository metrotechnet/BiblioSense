# Exemple d'utilisation du cache GPT - BiblioSense Phase 2

"""
Ce fichier montre les diff√©rentes fa√ßons d'utiliser le cache GPT 
avec get_catagories_with_gpt dans BiblioSense.
"""

from utils.gpt_cache import GPTCache
from utils.gpt_services import get_catagories_with_gpt, get_catagories_with_gpt_cached, create_cached_gpt_function

# ====================================================================
# M√âTHODE 1: Utilisation directe avec cache (comme dans app.py)
# ====================================================================

def example_direct_cache_usage(gpt_cache, taxonomy_data, openai_client):
    """Exemple d'utilisation directe du cache"""
    query = "livres de science fiction"
    
    # Appel avec cache
    categories = get_catagories_with_gpt_cached(
        text=query,
        taxonomy_data=taxonomy_data,
        openai_client=openai_client,
        gpt_cache=gpt_cache
    )
    
    return categories

# ====================================================================
# M√âTHODE 2: Utilisation avec factory function (recommand√©e)
# ====================================================================

def example_factory_usage():
    """Exemple avec factory function - plus propre"""
    
    # Configuration initiale (une seule fois)
    gpt_cache = GPTCache(max_size=500, ttl_seconds=3600)
    
    # Cr√©er la fonction pr√©-configur√©e
    cached_gpt_call = create_cached_gpt_function(
        gpt_cache=gpt_cache,
        openai_client=openai_client,  # Votre client OpenAI
        taxonomy_data=taxonomy_data   # Vos donn√©es de taxonomie
    )
    
    # Utilisation simple - juste passer le texte
    query = "romans d'aventure"
    categories = cached_gpt_call(query)
    
    return categories

# ====================================================================
# M√âTHODE 3: Int√©gration dans une classe (pour applications complexes)
# ====================================================================

class BiblioSenseGPTService:
    """Service GPT avec cache int√©gr√©"""
    
    def __init__(self, openai_client, taxonomy_data, cache_size=500, cache_ttl=3600):
        self.openai_client = openai_client
        self.taxonomy_data = taxonomy_data
        self.gpt_cache = GPTCache(max_size=cache_size, ttl_seconds=cache_ttl)
        
        # Cr√©er la fonction cach√©e
        self.classify_query = create_cached_gpt_function(
            self.gpt_cache, 
            self.openai_client, 
            self.taxonomy_data
        )
    
    def get_categories(self, user_query):
        """Interface simple pour classifier une requ√™te"""
        return self.classify_query(user_query)
    
    def clear_cache(self):
        """Vider le cache"""
        self.gpt_cache.cache.clear()
        self.gpt_cache.reset_stats()
    
    def get_cache_stats(self):
        """Obtenir les statistiques du cache"""
        return self.gpt_cache.get_stats()
    
    def is_cache_healthy(self, min_hit_rate=50.0):
        """V√©rifier si le cache est efficace"""
        stats = self.get_cache_stats()
        return stats['hit_rate_percent'] >= min_hit_rate

# ====================================================================
# M√âTHODE 4: D√©corateur pour cache automatique
# ====================================================================

def with_gpt_cache(cache_size=500, cache_ttl=3600):
    """D√©corateur pour ajouter automatiquement le cache √† une fonction GPT"""
    
    def decorator(gpt_function):
        cache = GPTCache(max_size=cache_size, ttl_seconds=cache_ttl)
        
        def wrapper(text, taxonomy_data, openai_client):
            # Essayer le cache
            cached_result = cache.get(text, taxonomy_data)
            if cached_result:
                return cached_result
            
            # Appeler la fonction originale
            result = gpt_function(text, taxonomy_data, openai_client)
            
            # Mettre en cache
            cache.set(text, taxonomy_data, result)
            
            return result
        
        # Ajouter les stats au wrapper
        wrapper.get_cache_stats = lambda: cache.get_stats()
        wrapper.clear_cache = lambda: cache.cache.clear()
        
        return wrapper
    
    return decorator

# Exemple d'utilisation du d√©corateur
@with_gpt_cache(cache_size=100, cache_ttl=1800)
def my_custom_gpt_function(text, taxonomy_data, openai_client):
    """Ma fonction GPT personnalis√©e avec cache automatique"""
    return get_catagories_with_gpt(text, taxonomy_data, openai_client)

# ====================================================================
# EXEMPLES D'UTILISATION DANS UNE APPLICATION FLASK
# ====================================================================

def flask_route_example():
    """Exemple d'utilisation dans une route Flask"""
    
    # M√©thode utilis√©e dans app.py (la plus simple)
    @app.route("/filter", methods=["POST"])
    def filter_with_cache():
        data = request.get_json()
        text = data.get("query", "")
        
        # Utilisation de la fonction pr√©-configur√©e
        categories = cached_gpt_classifier(text)  # Variable globale
        
        return jsonify(categories)

def performance_monitoring_example():
    """Exemple avec monitoring des performances"""
    
    cache = GPTCache(max_size=500)
    
    # Plusieurs requ√™tes pour tester le cache
    queries = [
        "science fiction",
        "romans historiques", 
        "science fiction",  # R√©p√©tition pour test cache
        "biographies",
        "romans historiques"  # R√©p√©tition pour test cache
    ]
    
    for i, query in enumerate(queries):
        print(f"\nRequ√™te {i+1}: {query}")
        
        start_time = time.time()
        result = get_catagories_with_gpt_cached(query, taxonomy_data, openai_client, cache)
        end_time = time.time()
        
        print(f"Temps: {end_time - start_time:.3f}s")
        
        # Stats du cache apr√®s chaque requ√™te
        stats = cache.get_stats()
        print(f"Hit rate: {stats['hit_rate_percent']:.1f}%")

# ====================================================================
# CONFIGURATION RECOMMAND√âE POUR PRODUCTION
# ====================================================================

def production_setup():
    """Configuration recommand√©e pour la production"""
    
    # Cache plus grand et TTL plus long pour la production
    production_cache = GPTCache(
        max_size=1000,      # Plus de requ√™tes en cache
        ttl_seconds=7200    # 2 heures de cache
    )
    
    # Fonction optimis√©e pour la production
    production_gpt_classifier = create_cached_gpt_function(
        gpt_cache=production_cache,
        openai_client=openai_client,
        taxonomy_data=taxonomy_data
    )
    
    return production_gpt_classifier

if __name__ == "__main__":
    print("üìö Exemples d'utilisation du cache GPT pour BiblioSense")
    print("Consultez ce fichier pour voir les diff√©rentes m√©thodes d'impl√©mentation.")

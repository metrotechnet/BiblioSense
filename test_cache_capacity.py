# Test de capacit√© avec cache - BiblioSense Phase 2

import time
import json
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from utils.gpt_cache import GPTCache
from utils.performance_monitor import PerformanceMonitor

def simulate_cache_performance():
    """Simule l'impact du cache sur la performance"""
    
    print("üß™ SIMULATION DE PERFORMANCE AVEC CACHE")
    print("=" * 60)
    
    # Cr√©er un cache de test
    cache = GPTCache(max_size=500, ttl_seconds=3600)
    monitor = PerformanceMonitor(max_users=200)
    
    # Simuler des requ√™tes populaires (repr√©sentent 80% du trafic)
    popular_queries = [
        "science fiction",
        "romans fran√ßais",
        "histoire de France", 
        "philosophie",
        "biographies",
        "litt√©rature classique",
        "romans d'aventure",
        "po√©sie fran√ßaise"
    ]
    
    # Simuler des donn√©es de taxonomie
    test_taxonomy = {"test": "data"}
    
    # Mock response pour simulation
    mock_response = {
        "Description": "Test query",
        "Mots-cl√©s": {"titre": ["test"]},
        "Taxonomie": {"Sciences": {"Physique": ["test"]}}
    }
    
    # Pr√©-remplir le cache avec les requ√™tes populaires
    print("üìù Pr√©-remplissage du cache avec requ√™tes populaires...")
    for query in popular_queries:
        cache.set(query, test_taxonomy, mock_response)
    
    print(f"   ‚úÖ Cache pr√©-rempli avec {len(popular_queries)} requ√™tes")
    
    # Simuler diff√©rents sc√©narios
    scenarios = [
        ("Cache √† froid", 0.0),      # 0% hit rate
        ("Cache ti√®de", 0.5),        # 50% hit rate
        ("Cache chaud", 0.8),        # 80% hit rate
        ("Cache optimal", 0.95)      # 95% hit rate
    ]
    
    results = {}
    
    for scenario_name, hit_rate in scenarios:
        print(f"\nüéØ Test du sc√©nario: {scenario_name} ({hit_rate*100:.0f}% hit rate)")
        
        # Simuler 100 requ√™tes avec le hit rate donn√©
        start_time = time.time()
        total_requests = 100
        
        for i in range(total_requests):
            user_id = f"user_{i % 20}"  # 20 utilisateurs simul√©s
            
            if i / total_requests < hit_rate:
                # Cache hit - requ√™te populaire
                query = popular_queries[i % len(popular_queries)]
                request_start = time.time()
                result = cache.get(query, test_taxonomy)
                request_time = time.time() - request_start
            else:
                # Cache miss - nouvelle requ√™te
                query = f"requ√™te unique {i}"
                request_start = time.time()
                result = cache.get(query, test_taxonomy)  # Sera None
                # Simuler appel GPT (2-5s)
                time.sleep(0.01)  # Simulation rapide
                cache.set(query, test_taxonomy, mock_response)
                request_time = time.time() - request_start
            
            # Enregistrer dans le monitor
            monitor.track_request(user_id, request_time)
        
        total_time = time.time() - start_time
        
        # Calculer les statistiques
        cache_stats = cache.get_stats_fast()
        perf_stats = monitor.get_stats(safe_mode=True)
        
        avg_request_time = total_time / total_requests
        requests_per_second = total_requests / total_time
        
        results[scenario_name] = {
            'hit_rate': cache_stats['hit_rate_percent'],
            'avg_request_time': avg_request_time,
            'requests_per_second': requests_per_second,
            'total_time': total_time,
            'active_users': perf_stats['active_users']
        }
        
        print(f"   üìä R√©sultats:")
        print(f"      Hit rate r√©el: {cache_stats['hit_rate_percent']:.1f}%")
        print(f"      Temps moyen/requ√™te: {avg_request_time:.3f}s")
        print(f"      Requ√™tes/seconde: {requests_per_second:.1f}")
        print(f"      Utilisateurs actifs: {perf_stats['active_users']}")
    
    # Calculer l'impact sur la capacit√©
    print(f"\nüìà IMPACT SUR LA CAPACIT√â")
    print("-" * 40)
    
    base_rps = results["Cache √† froid"]["requests_per_second"]
    
    for scenario_name, data in results.items():
        rps = data["requests_per_second"]
        improvement = rps / base_rps if base_rps > 0 else 1
        estimated_capacity = int(rps * 2)  # Estimation simple
        
        print(f"{scenario_name}:")
        print(f"   Req/s: {rps:.1f} ({improvement:.1f}x plus rapide)")
        print(f"   Capacit√© estim√©e: {estimated_capacity} utilisateurs")
    
    return results

def test_concurrent_load():
    """Test de charge concurrent"""
    
    print(f"\nüöÄ TEST DE CHARGE CONCURRENT")
    print("=" * 40)
    
    cache = GPTCache(max_size=100, ttl_seconds=3600)
    monitor = PerformanceMonitor()
    
    # Pr√©-remplir avec quelques requ√™tes
    popular_queries = ["science", "histoire", "roman"]
    test_taxonomy = {"test": "data"}
    mock_response = {"result": "test"}
    
    for query in popular_queries:
        cache.set(query, test_taxonomy, mock_response)
    
    def simulate_user_requests(user_id, num_requests=5):
        """Simule les requ√™tes d'un utilisateur"""
        request_times = []
        
        for i in range(num_requests):
            start_time = time.time()
            
            # 70% de chance d'utiliser une requ√™te populaire
            if i % 10 < 7:
                query = popular_queries[i % len(popular_queries)]
            else:
                query = f"unique_query_{user_id}_{i}"
            
            # Simuler la requ√™te
            result = cache.get(query, test_taxonomy)
            if not result:
                # Cache miss - simuler GPT call
                time.sleep(0.001)  # Simulation tr√®s rapide
                cache.set(query, test_taxonomy, mock_response)
            
            request_time = time.time() - start_time
            request_times.append(request_time)
            monitor.track_request(f"user_{user_id}", request_time)
            
            # Petite pause entre les requ√™tes
            time.sleep(0.01)
        
        return {
            'user_id': user_id,
            'avg_time': sum(request_times) / len(request_times),
            'total_requests': len(request_times)
        }
    
    # Test avec diff√©rents nombres d'utilisateurs concurrents
    user_counts = [5, 10, 20, 30]
    
    for num_users in user_counts:
        print(f"\nüë• Test avec {num_users} utilisateurs concurrents:")
        
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=num_users) as executor:
            futures = [
                executor.submit(simulate_user_requests, i, 5) 
                for i in range(num_users)
            ]
            
            results = [future.result() for future in as_completed(futures)]
        
        total_time = time.time() - start_time
        total_requests = sum(r['total_requests'] for r in results)
        
        # Statistiques
        perf_stats = monitor.get_stats(safe_mode=True)
        cache_stats = cache.get_stats_fast()
        
        print(f"   ‚è±Ô∏è  Temps total: {total_time:.2f}s")
        print(f"   üìä Total requ√™tes: {total_requests}")
        print(f"   üîÑ Hit rate: {cache_stats['hit_rate_percent']:.1f}%")
        print(f"   üë§ Utilisateurs actifs: {perf_stats['active_users']}")
        print(f"   üìà Req/s: {total_requests/total_time:.1f}")
        
        # V√©rifier si le syst√®me reste stable
        avg_response_time = perf_stats.get('avg_response_time', 0)
        if avg_response_time < 1.0:
            print(f"   ‚úÖ Stable (temps r√©ponse: {avg_response_time:.3f}s)")
        else:
            print(f"   ‚ö†Ô∏è  Risque de surcharge (temps r√©ponse: {avg_response_time:.3f}s)")

if __name__ == "__main__":
    print("üéØ ANALYSE DE PERFORMANCE CACHE - BIBLIOSENSE PHASE 2")
    print("=" * 70)
    
    # Test de performance du cache
    simulate_cache_performance()
    
    # Test de charge concurrent
    test_concurrent_load()
    
    print(f"\nüéâ CONCLUSION:")
    print("Le cache GPT am√©liore drastiquement la capacit√© utilisateur.")
    print("Avec 80% de hit rate, la capacit√© peut √™tre multipli√©e par 5-10x.")
    print("L'optimisation du cache est cruciale pour la mont√©e en charge.")

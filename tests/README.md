# üß™ Scripts de Validation pour get_keywords_with_gpt

Ce dossier contient plusieurs scripts pour tester et valider la fonction `get_keywords_with_gpt` de BiblioSense.

## üìã Vue d'ensemble

La validation de `get_keywords_with_gpt` se fait en plusieurs √©tapes :

1. **G√©n√©ration des r√©sultats attendus** - Analyse automatique des requ√™tes
2. **Tests avec l'API r√©elle** - Appels √† OpenAI GPT
3. **Validation comparative** - Comparaison des r√©sultats r√©els vs attendus

## üìÇ Fichiers cr√©√©s

### Scripts principaux

- **`generate_validation_template.py`** ‚úÖ
  - G√©n√®re automatiquement les r√©sultats attendus pour toutes les requ√™tes
  - Analyse les patterns dans `EXEMPLES_REQUETES.md`
  - Cr√©e `keywords_validation_expected.json`

- **`test_keywords_real.py`** üîë
  - Script pour tester avec la vraie API OpenAI
  - Utilise la m√™me logique d'initialisation que `app.py`
  - N√©cessite `OPENAI_API_KEY` ou Google Cloud Secret Manager

- **`validate_keywords.py`** ‚úÖ
  - Compare les r√©sultats r√©els avec les attendus
  - Syst√®me de scoring sur 100 points
  - G√©n√®re des rapports de validation d√©taill√©s

### Scripts de support

- **`test_keywords_validation.py`** - Version compl√®te avec tous les cas de test
- **`simple_test.py`** - Test basique simplifi√©

## üöÄ Comment utiliser

### √âtape 1 : G√©n√©rer les r√©sultats attendus

```bash
cd tests
python generate_validation_template.py
```

Ce script va :
- Extraire toutes les requ√™tes de `../EXEMPLES_REQUETES.md`
- Analyser chaque requ√™te pour pr√©dire le r√©sultat attendu
- Cr√©er le fichier `keywords_validation_expected.json`

**R√©sultat** : 82 cas de test avec r√©sultats attendus

### √âtape 2 : Tester avec l'API r√©elle (optionnel)

```bash
# D√©finir la cl√© API
$env:OPENAI_API_KEY = "your-api-key-here"

# Ou configurer Google Cloud
$env:GOOGLE_CLOUD_PROJECT = "your-project-id"

# Lancer le test
python test_keywords_real.py
```

### √âtape 3 : Valider les r√©sultats

```bash
python validate_keywords.py
```

Ce script va :
- Charger les r√©sultats attendus
- Simuler ou utiliser les vrais r√©sultats GPT
- Comparer et scorer chaque test
- G√©n√©rer un rapport de validation

## üìä Syst√®me de scoring

Chaque test est not√© sur **100 points** :

- **40 points** : Champ correct (auteur, titre, categorie, etc.)
- **40 points** : Qualit√© des mots-cl√©s (pertinence, compl√©tude)
- **20 points** : Format JSON correct

**Seuil de r√©ussite** : 60/100

## üìà R√©sultats de validation

### √âtat actuel (simulation)
- ‚úÖ **Tests r√©ussis** : 4/6 (66.7%)
- üìà **Score moyen** : 74.3/100
- üìä **Performance** : Acceptable

### Cas probl√©matiques identifi√©s

1. **Erreur de champ** : 
   - `"Le Petit Prince"` ‚Üí Identifi√© comme auteur au lieu de titre
   
2. **Format incorrect** :
   - Retour de string au lieu de liste pour les mots-cl√©s

## üîß Am√©liorer les r√©sultats

### 1. Ajuster le prompt GPT

Dans `utils/gpt_services.py`, modifier le prompt pour :
- Mieux distinguer titres vs auteurs
- Renforcer les instructions de format
- Ajouter des exemples explicites

### 2. Post-traitement

Ajouter une validation du format de sortie :
```python
def validate_format(result):
    if "keywords" not in result:
        return {"keywords": {"categorie": ["general"]}}
    
    for field, keywords in result["keywords"].items():
        if isinstance(keywords, str):
            result["keywords"][field] = [keywords]
    
    return result
```

### 3. R√®gles sp√©cifiques

Impl√©menter des r√®gles pour les cas √©vidents :
- Titres connus (Le Petit Prince, 1984, etc.)
- Auteurs c√©l√®bres (Victor Hugo, etc.)
- Patterns de langue ("en anglais", "en fran√ßais")

## üìÅ Fichiers g√©n√©r√©s

- `keywords_validation_expected.json` - R√©sultats attendus (82 cas)
- `validation_results.json` - R√©sultats d√©taill√©s des tests
- `test_results.json` - R√©sultats bruts des appels GPT

## üéØ Prochaines √©tapes

1. **Examiner les r√©sultats** dans `keywords_validation_expected.json`
2. **Corriger manuellement** les cas mal analys√©s
3. **Tester avec l'API r√©elle** quand disponible
4. **It√©rer sur le prompt** bas√© sur les r√©sultats
5. **Automatiser** les tests dans la CI/CD

## üí° Notes importantes

- Les r√©sultats attendus sont g√©n√©r√©s automatiquement et peuvent n√©cessiter des ajustements manuels
- La simulation est utilis√©e en l'absence de cl√© API OpenAI
- Le scoring est indicatif et peut √™tre ajust√© selon les priorit√©s business
- Les tests couvrent diff√©rents types de requ√™tes : auteurs, genres, langues, formats, etc.

## üÜò D√©pannage

### Erreur : "OPENAI_API_KEY non d√©finie"
```bash
# Windows PowerShell
$env:OPENAI_API_KEY = "sk-..."

# Ou cr√©er un fichier .env
echo "OPENAI_API_KEY=sk-..." > .env
```

### Erreur : "Fichier non trouv√©"
V√©rifiez que `EXEMPLES_REQUETES.md` existe dans le r√©pertoire courant.

### Erreur d'import
```bash
pip install openai
```
